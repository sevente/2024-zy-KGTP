目前已经完成的功能：
- [x] inference推理/decode中的数据预处理
- [x] 基于hugging face预训练的bart进行data-to-text generation
- [x] 基于本文finetuned之后的bart进行data-to-text generation

# 激活 conda 环境
```
conda activate /home/sdb/xx/path/anaconda_path/llm_prince
```

# 数据预处理
### huggingface官方webnlg data 数据下载
下载数据, 将原始数据中的text 和 data保存为文本格式，其实存在多个triples三元组的，利用 > 符号隔开，存在多个reference 文本的，
将文本和三元组作为一对单独占一行。 SPLIT_NAME 可以是train, test, dev  ---> 输出webnlg-train.tex，webnlg-test.tex，webnlg-dev.tex
```angular2html
python download-webnlg.py SPLIT_NAME
```

### mvp 项目的webnlg，webnlg2， e2e, dart数据集， 数据格式转换
将数据保存到mvp_dataset数据集中，保存为xx.tex格式。 多个triples 通过 "▸" 进行拼接
```
python critics/datasets_generators/convert_mvp_format_to_critic.py
```

### 训练用的数据(huggingface官方webnlg data)
以下将生成大量训练数据，采用将完整的句子从长度为1 到 完整句子进行拼接，构造了大量负样本
```
    python critics/datasets_generators/gen_train_onlystop.py train 或者 test 或者 dev
```
以下将生成较少量的训练数据，采用将完整的句子和完整句子进行拼接
```
    python critics/datasets_generators/gen_train.py train 或者 test 或者 dev
```
然后将生成的csv 文件复制到对应的实验文件夹，dataset_only_stop_gen 或者 dataset_gen， 文件命名为 train.csv, test.csv, dev.csv

### 训练用的数据(mvp 项目的webnlg，webnlg2， e2e, dart数据集)
以下将生成大量训练数据，采用将完整的句子从长度为1 到 完整句子进行拼接，构造了大量负样本
```
    python critics/datasets_generators/gen_train_onlystop.py train webnlg 或者 webnlg2 或者 e2e 或者 dart
```
以下将生成较少量的训练数据，采用将完整的句子和完整句子进行拼接
```
    python critics/datasets_generators/gen_train.py train webnlg 或者 webnlg2 或者 e2e 或者 dart
```
生成的文件输出到 mvp_dataset下对应的文件夹 dataset_only_stop_gen 或者 dataset_gen

### decode 用的数据 (huggingface官方webnlg data)
将数据转换为json格式，用于 decode ， json包含 "in" 和 "out" 两个features. 
```
python3 bin/preprocess.py --dataset webnlg --output_dir data/webnlg --mode linearize_triples

代码调用链： class WebNLG(Dataset):
```
将数据格式:
```
{
    "language": "en",
    "values": [
        {
            "input": [
                "Aarhus_Airport | cityServed | \"Aarhus, Denmark\""
            ],
            "target": [
                "The Aarhus is the airport of Aarhus, Denmark.",
                "Aarhus Airport serves the city of Aarhus, Denmark."
            ],
            "category": "Airport",
            "id": 0,
            "webnlg-id": "train/Airport/1/Id1"
        },
......
```
转换为：
```
{
    "data": [
        {
            "in": "Aarhus Airport | city served | Aarhus, Denmark",
            "out": "The Aarhus is the airport of Aarhus, Denmark."
        },
        {
            "in": "Aarhus Airport | city served | Aarhus, Denmark",
            "out": "Aarhus Airport serves the city of Aarhus, Denmark."
        },
.....

```

### decode 用的数据 (mvp 项目的webnlg，webnlg2， e2e, dart数据集)
将数据转换为json格式，用于 decode ， json包含 "in" 和 "out" 两个features. 
```
python3 bin/preprocess_mvp.py 
```

# Training the critic
注意： critic 和 实际用到的LM model不是一个模型，本文用xlm-roberta-base作为critic。

Put the generated training data into outdir. The outdir directory should contain 3 files: train.csv, test.csv, and dev.csv with the training/test/validation data (these files should be generated by gen_train*.py scripts -- see above)

```
python critics/run.py --batch_size 32 --outdir dataset_gen --model MLPSELU --lr 1e-5 --loss Logistic --pretrained_model "/home/sdb/xx/path/modelZooHuggingFace/xlm-roberta-base" --ckpt_path "./dataset_gen/my_cpts/epoch=0-step=2113.ckpt"
```
即利用自己构造的正负样本，进行一个二分类的训练。

```
python critics/run.py --batch_size 32 --outdir mvp_dataset/webnlg/dataset_gen --model MLPSELU --lr 1e-5 --loss Logistic --pretrained_model "/home/sdb/xx/path/modelZooHuggingFace/xlm-roberta-base"
```


# Critic-aware decoding (--checkpoint 为fintune之后的bart-base语言模型，webnlg-based bart从本repo提供的链接下载，--ciritc_ckpt 为critic的模型)

Put the checkpoint of fine-tuned LM model into `experiments/webnlg/CHECKPOINT_NAME` path. Our BART-based LM model fine-tuned on WebNLG can be downloaded from [here](http://ufallab.ms.mff.cuni.cz/~lango/webnlg-model.ckpt).
The checkpoint of a trained critic should be located in `CRITIC_CHECKPOINT_NAME`. The name of the output file with the decoded text is specified by `FILE_NAME`.
```
python3 ./bin/decode.py \
    --experiment webnlg \
    --checkpoint webnlg-model.ckpt \
    --in_dir data/webnlg \
    --split test \
    --accelerator gpu \
    --devices 1 \
    --beam_size 1 \
    --condition_lambda 0.25 \
    --critic_top_k 5 \
    --linear_warmup \
    --batch_size 8\
    --critic_ckpt  "./dataset_gen/my_cpts/epoch=0-step=8452.ckpt" \
    --out_filename test --wrapper classifier --load_in_8bit
```

```
python3 ./bin/decode.py --experiment webnlg_mvp --checkpoint webnlg-model.ckpt --in_dir mvp_dataset/webnlg_for_decode --split test --accelerator gpu --devices 1 --beam_size 1 --condition_lambda 0.25 --critic_top_k 5 --linear_warmup --batch_size 8 --critic_ckpt  "./mvp_dataset/webnlg/dataset_gen/my_cpts/epoch=0-step=8228.ckpt" --out_filename test --wrapper classifier --load_in_8bit
```

# 基于微调后的T5模型, Critic-aware decoding
```
python3 ./bin/decode.py --experiment webnlg_mvp_t5FT --fintuned_model "/home/sdb/xx/path/LLM/SFTData2Text/Benchmark/HuggingFace_Data2Text/webnlg_Pretrained_T5/checkpoint-5000" --in_dir mvp_dataset/webnlg_for_decode --split test --accelerator gpu --devices 1 --beam_size 1 --condition_lambda 0.25 --critic_top_k 5 --linear_warmup --batch_size 8 --critic_ckpt  "./mvp_dataset/webnlg/dataset_gen/my_cpts/epoch=0-step=8228.ckpt" --out_filename test --wrapper classifier
python3 ./bin/decode.py --experiment webnlg2_mvp_t5FT --fintuned_model "/home/sdb/xx/path/LLM/SFTData2Text/Benchmark/HuggingFace_Data2Text/webnlg2_Pretrained_T5/checkpoint-5500" --in_dir mvp_dataset/webnlg2_for_decode --split test --accelerator gpu --devices 1 --beam_size 1 --condition_lambda 0.25 --critic_top_k 5 --linear_warmup --batch_size 8 --critic_ckpt  "./mvp_dataset/webnlg2/dataset_gen/my_cpts/epoch=0-step=8228.ckpt" --out_filename test_ --wrapper classifier
python3 ./bin/decode.py --experiment e2e_mvp_t5FT --fintuned_model "/home/sdb/xx/path/LLM/SFTData2Text/Benchmark/HuggingFace_Data2Text/e2e_Pretrained_T5/checkpoint-5000" --in_dir mvp_dataset/e2e_for_decode --split test --accelerator gpu --devices 1 --beam_size 1 --condition_lambda 0.25 --critic_top_k 5 --linear_warmup --batch_size 8 --critic_ckpt  "./mvp_dataset/e2e/dataset_gen/my_cpts/epoch=0-step=8228.ckpt" --out_filename test --wrapper classifier
python3 ./bin/decode.py --experiment dart_mvp_t5FT --fintuned_model "/home/sdb/xx/path/LLM/SFTData2Text/Benchmark/HuggingFace_Data2Text/dart_Pretrained_T5/checkpoint-8500" --in_dir mvp_dataset/dart_for_decode --split test --accelerator gpu --devices 1 --beam_size 1 --condition_lambda 0.25 --critic_top_k 5 --linear_warmup --batch_size 8 --critic_ckpt  "./mvp_dataset/dart/dataset_gen/my_cpts/epoch=0-step=8228.ckpt" --out_filename test --wrapper classifier
```

# 基于微调后的mvp模型, Critic-aware decoding
```
python3 ./bin/decode.py --experiment webnlg_mvp_MVPFT --fintuned_model "/home/sdb/xx/path/LLM/SFTData2Text/Benchmark/HuggingFace_Data2Text_MVP/webnlg_Pretrained_MVPD2T/checkpoint-4200" --in_dir mvp_dataset/webnlg_for_decode --split test --accelerator gpu --devices 1 --beam_size 1 --condition_lambda 0.25 --critic_top_k 5 --linear_warmup --batch_size 8 --critic_ckpt  "./mvp_dataset/webnlg/dataset_gen/my_cpts/epoch=0-step=8228.ckpt" --out_filename test --wrapper classifier
python3 ./bin/decode.py --experiment webnlg2_mvp_MVPFT --fintuned_model "/home/sdb/xx/path/LLM/SFTData2Text/Benchmark/HuggingFace_Data2Text_MVP/webnlg2_Pretrained_MVPD2T/checkpoint-1400" --in_dir mvp_dataset/webnlg2_for_decode --split test --accelerator gpu --devices 1 --beam_size 1 --condition_lambda 0.25 --critic_top_k 5 --linear_warmup --batch_size 8 --critic_ckpt  "./mvp_dataset/webnlg2/dataset_gen/my_cpts/epoch=0-step=8228.ckpt" --out_filename test_ --wrapper classifier
python3 ./bin/decode.py --experiment e2e_mvp_MVPFT --fintuned_model "/home/sdb/xx/path/LLM/SFTData2Text/Benchmark/HuggingFace_Data2Text_MVP/e2e_Pretrained_MVPD2T/checkpoint-200" --in_dir mvp_dataset/e2e_for_decode --split test --accelerator gpu --devices 1 --beam_size 1 --condition_lambda 0.25 --critic_top_k 5 --linear_warmup --batch_size 8 --critic_ckpt  "./mvp_dataset/e2e/dataset_gen/my_cpts/epoch=0-step=8228.ckpt" --out_filename test --wrapper classifier
python3 ./bin/decode.py --experiment dart_mvp_MVPFT --fintuned_model "/home/sdb/xx/path/LLM/SFTData2Text/Benchmark/HuggingFace_Data2Text_MVP/dart_Pretrained_MVPD2T/checkpoint-9600" --in_dir mvp_dataset/dart_for_decode --split test --accelerator gpu --devices 1 --beam_size 1 --condition_lambda 0.25 --critic_top_k 5 --linear_warmup --batch_size 8 --critic_ckpt  "./mvp_dataset/dart/dataset_gen/my_cpts/epoch=0-step=8228.ckpt" --out_filename test --wrapper classifier
```


# metrics
运行 metrics.py 进行评估


# 基于hugging face预训练bart-base, 进行data-to-text generation
关键在于理解 inference.py中的 CriticGenDataInferenceModule 类中的predict_step函数，可以参考 test_bart.py

### huggingface官方webnlg data
```
python3 ./bin/decode.py --model_name facebook/bart-base --experiment webnlg --in_dir data/webnlg --split test --accelerator gpu --devices 1 --beam_size 1 --condition_lambda 1.0 --critic_top_k 5  --batch_size 8 --out_filename FILE_NAME --wrapper data
```

### mvp 项目的 webnlg，webnlg2， e2e, dart数据集)
```
python3 ./bin/decode.py --model_name facebook/bart-base --experiment webnlg_mvp --in_dir mvp_dataset/webnlg_for_decode --split test --accelerator gpu --devices 1 --beam_size 1 --condition_lambda 1.0 --critic_top_k 5  --batch_size 8 --out_filename test --wrapper data
```

# 基于本文微调之后的checkpoint（语言模型，本文为bart-base), 进行data-to-text generation, 无 critic
```
python3 ./bin/decode.py \
    --experiment webnlg \
    --checkpoint webnlg-model.ckpt \
    --in_dir data/webnlg \
    --split test \
    --accelerator gpu \
    --devices 1 \
    --beam_size 1 \
    --condition_lambda 1.0 \
    --critic_top_k 5 \
    --batch_size 8 \
    --out_filename FILE_NAME \
    --wrapper data
```
上述代码可能会报错，将wrapper 去除后再执行
```
python3 ./bin/decode.py \
    --experiment webnlg \
    --checkpoint webnlg-model.ckpt \
    --in_dir data/webnlg \
    --split test \
    --accelerator gpu \
    --devices 1 \
    --beam_size 1 \
    --condition_lambda 1.0 \
    --critic_top_k 5 \
    --batch_size 8 \
    --out_filename test
```

# 以下是github原始仓库的代码

# Critic-Driven Decoding for Mitigating Hallucinations in Data-to-text Generation

## Critic classifier training
If you want to use WebNLG data, you can download it with
```
python critics/dataset_generators/download-webnlg.py SPLIT_NAME
```

### Generating data

- ver 1. critic (base)
```
python critics/dataset_generators/gen_train_onlystop.py SPLIT_NAME
```
`SPLIT_NAME` is a placeholder for "train", "test", and "dev". To generate all necessary data, you should run the command three times i.e.
```
python critics/dataset_generators/gen_train_onlystop.py train
python critics/dataset_generators/gen_train_onlystop.py test
python critics/dataset_generators/gen_train_onlystop.py dev
```

- ver 2. critic (base with full sentences)
```
python critics/dataset_generators/gen_train.py SPLIT_NAME 
```
- ver 3. critic (vanilla LM)
```
python3 ./bin/decode.py \
    --model_name facebook/bart-base \
    --experiment webnlg \
    --in_dir data/webnlg \
    --split SPLIT_NAME \
    --accelerator gpu \
    --devices 1 \
    --beam_size 1 \
    --condition_lambda 1.0 \
    --critic_top_k 5 \
    --batch_size 8\
    --out_filename FILE_NAME --wrapper data --load_in_8bit
 ```
where `--model_name` is the name of LM used to generate data from huggingface (here: `facebook/bart-base`)
```
python critics/dataset_generators/gen_train_fromLM.py SPLIT_NAME FILE_NAME-data
```
- ver 4. critic (fine-tuned LM)

Put the checkpoint of fine-tuned language model into `experiments/webnlg/CHECKPOINT_NAME` path. Our BART-based LM model fine-tuned on WebNLG can be downloaded from `https://we.tl/t-1aufs3tnyS`
```
python3 ./bin/decode.py \
    --experiment webnlg \
    --checkpoint CHECKPOINT_NAME \
    --in_dir data/webnlg \
    --split SPLIT_NAME \
    --accelerator gpu \
    --devices 1 \
    --beam_size 1 \
    --condition_lambda 1.0 \
    --critic_top_k 5 \
    --batch_size 8\
    --out_filename FILE_NAME --wrapper data --load_in_8bit

python critics/dataset_generators/gen_train_fromLM.py SPLIT_NAME FILE_NAME-data
```
- ver 5. critic (fine-tuned LM with full sentences)
```
python3 ./bin/decode.py \
    --experiment webnlg \ 
    --checkpoint CHECKPOINT_NAME \
    --in_dir data/webnlg \
    --split SPLIT_NAME \
    --accelerator gpu \
    --devices 1 \
    --beam_size 1 \
    --condition_lambda 1.0 \
    --critic_top_k 5 \
    --batch_size 8\
    --out_filename FILE_NAME --wrapper data-full --load_in_8bit

python critics/dataset_generators/gen_train_fromLM.py SPLIT_NAME FILE_NAME-data
```
## Training the critic
Put the generated training data into OUT_DIR. The OUT_DIR directory should contain 3 files: `train.csv`, `test.csv`, and `dev.csv` with the training/test/validation data (these files should be generated by `gen_train*.py` scripts -- see above)
```
python critics/run.py --batch_size 32 --outdir OUT_DIR --model MLPSELU --lr 1e-5
```

# Critic-aware decoding

Put the checkpoint of fine-tuned LM model into `experiments/webnlg/CHECKPOINT_NAME` path. Our BART-based LM model fine-tuned on WebNLG can be downloaded from [here](http://ufallab.ms.mff.cuni.cz/~lango/webnlg-model.ckpt).
The checkpoint of a trained critic should be located in `CRITIC_CHECKPOINT_NAME`. The name of the output file with the decoded text is specified by `FILE_NAME`.
```
python3 ./bin/decode.py \
    --experiment webnlg \
    --checkpoint LM_CHECKPOINT_NAME \
    --in_dir data/webnlg \
    --split test \
    --accelerator gpu \
    --devices 1 \
    --beam_size 1 \
    --condition_lambda 0.25 \
    --critic_top_k 5 \
    --linear_warmup \
    --batch_size 8\
    --critic_ckpt CRITIC_CHECKPOINT_NAME \
    --out_filename FILE_NAME --wrapper classifier --load_in_8bit
```
